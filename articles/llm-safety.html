<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Rethinking LLM Safety - Rohan Saxena</title>
    <link rel="stylesheet" href="../css/style.css">
    <style>
        .article-container {
            max-width: 800px;
            margin: 80px auto;
            padding: 40px;
            transition: all 0.3s ease;
        }
        .article-header {
            margin-bottom: 40px;
        }
        .article-title {
            font-size: 2.5em;
            color: #2c2c2c;
            margin-bottom: 16px;
            transition: color 0.3s ease;
        }
        .article-meta {
            color: #888;
            font-size: 0.95em;
            transition: color 0.3s ease;
        }
        .article-content {
            line-height: 1.8;
            color: #444;
            transition: color 0.3s ease;
        }
        .article-content h2 {
            margin-top: 40px;
            font-size: 1.6em;
            color: #2c2c2c;
            transition: color 0.3s ease;
        }
        .article-content p {
            margin-bottom: 20px;
        }
        .back-link {
            display: inline-block;
            margin-top: 40px;
            color: #2563eb;
            text-decoration: none;
            transition: color 0.3s ease;
        }
        .back-link:hover {
            text-decoration: underline;
        }
        
        /* AI Mode Styles */
        body.ai-mode {
            background: #0a0a0a;
        }
        body.ai-mode .article-container {
            background: #0a0a0a;
        }
        body.ai-mode .article-title {
            color: #e0e0e0;
        }
        body.ai-mode .article-title::before {
            content: '> ';
            color: #666;
        }
        body.ai-mode .article-meta {
            color: #666;
            font-family: 'Courier New', monospace;
        }
        body.ai-mode .article-content {
            color: #aaa;
            font-family: 'Courier New', monospace;
            font-size: 0.95em;
        }
        body.ai-mode .article-content h2 {
            color: #e0e0e0;
        }
        body.ai-mode .article-content h2::before {
            content: '## ';
            color: #666;
        }
        body.ai-mode .back-link {
            color: #60a5fa;
            font-family: 'Courier New', monospace;
        }
        body.ai-mode .back-link:hover {
            color: #93c5fd;
        }
    </style>
</head>
<body>
    <div class="article-container">
        <div class="article-header">
            <h1 class="article-title">Rethinking LLM Safety Through Mechanistic Lenses</h1>
            <div class="article-meta">December 2024 • 8 min read • Rohan Saxena</div>
        </div>
        
        <div class="article-content">
            <p>
                Large Language Models have transformed how we interact with AI, but their safety mechanisms remain largely opaque. Traditional approaches to LLM safety focus on output filtering and reinforcement learning from human feedback (RLHF), but what if we could peek inside the model's reasoning process itself?
            </p>
            
            <h2>The Black Box Problem</h2>
            <p>
                Current safety measures treat LLMs as black boxes, evaluating only their outputs. This approach misses crucial information about how models arrive at potentially harmful conclusions. By the time we detect a safety issue in the output, the damage is done.
            </p>
            
            <h2>Mechanistic Interpretability as a Solution</h2>
            <p>
                Mechanistic interpretability offers a different approach: understanding the internal circuits and representations that lead to specific behaviors. By identifying which attention heads and layers activate during harmful content generation, we can intervene at the source rather than filtering outputs.
            </p>
            
            <p>
                In our recent research, we discovered that harmful content generation often involves specific attention patterns that can be detected and redirected before completion. This opens up new possibilities for real-time safety interventions that don't rely on post-hoc filtering.
            </p>
            
            <h2>Causal Interventions</h2>
            <p>
                Using causal intervention methods, we can test hypotheses about model behavior by directly manipulating internal activations. This allows us to answer questions like: "What would the model output if we removed the representation of violence from this layer?"
            </p>
            
            <p>
                These techniques reveal that safety isn't just about what the model says, but about the internal reasoning pathways it activates. By understanding and modifying these pathways, we can create more robust and transparent safety mechanisms.
            </p>
            
            <h2>The Path Forward</h2>
            <p>
                The future of LLM safety lies not in building higher walls around model outputs, but in understanding and guiding the internal processes that generate those outputs. This requires interdisciplinary collaboration between AI safety researchers, mechanistic interpretability experts, and domain specialists.
            </p>
            
            <p>
                As we continue to deploy increasingly powerful language models, the stakes for getting safety right only grow higher. Mechanistic approaches offer a promising path toward truly understanding and controlling AI behavior from the inside out.
            </p>
        </div>
        
        <a href="../index.html" class="back-link">← Back to Home</a>
    </div>

    <!-- Mode Toggle -->
    <div class="mode-toggle">
        <button class="mode-btn human active">HUMAN</button>
        <button class="mode-btn ai">AI</button>
    </div>

    <script>
        // Mode toggle functionality for article page
        const humanBtn = document.querySelector('.mode-btn.human');
        const aiBtn = document.querySelector('.mode-btn.ai');
        const body = document.body;
        
        // Check for saved mode preference
        const savedMode = localStorage.getItem('viewMode') || 'human';
        if (savedMode === 'ai') {
            body.classList.add('ai-mode');
            aiBtn.classList.add('active');
            humanBtn.classList.remove('active');
        }
        
        humanBtn.addEventListener('click', () => {
            body.classList.remove('ai-mode');
            humanBtn.classList.add('active');
            aiBtn.classList.remove('active');
            localStorage.setItem('viewMode', 'human');
        });
        
        aiBtn.addEventListener('click', () => {
            body.classList.add('ai-mode');
            aiBtn.classList.add('active');
            humanBtn.classList.remove('active');
            localStorage.setItem('viewMode', 'ai');
        });
    </script>
</body>
</html>